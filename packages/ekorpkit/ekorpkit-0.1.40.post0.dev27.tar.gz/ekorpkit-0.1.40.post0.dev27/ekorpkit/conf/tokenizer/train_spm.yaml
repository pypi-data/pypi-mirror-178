_target_: ekorpkit.tokenizers.trainer.train_tokenizer
model_prefix:
# output model name prefix. <model_name>.model and <model_name>.vocab are generated.
input_files:
# one-sentence-per-line raw corpus file.
# No need to run tokenizer, normalizer or preprocessor.
# By default, SentencePiece normalizes the input with Unicode NFKC.
# You can pass a comma-separated list of files.
input_dir:
output_dir: tokenizers
vocab_size: 30000
model_type: unigram
# model type. Choose from unigram (default), bpe, char, or word.
# The input sentence must be pretokenized when using word type.
trainer_type: spm
character_coverage: 1.0
# amount of characters covered by the model,
# good defaults are: 0.9995 for languages with rich character set like Japanese or Chinese
# and 1.0 for other languages with small character set.
num_workers: ${oc.select:num_workers, 1}
train_extremely_large_corpus: false
# If true, train the model using the --input_sentence_size option.
# This option is useful when the input corpus is extremely large.
# The input_sentence_size is set to 100M by default.
# If you want to change the value, set the --input_sentence_size option.
# For example, --input_sentence_size=1000000000 means 1 billion.
# The input_sentence_size must be larger than 1M.
project_dir: ${oc.select:dir.project, null}
verbose: ${oc.select:verbose, false}
# control_symbols: ""
# input_sentence_size:
# shuffle_input_sentence: true
# add_dummy_prefix: false
# bos_id: -1
# eos_id: 1
# unk_id: 2
# pad_id: 0
